{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1hnjWbnWF7-NCD1UkMFyQUtO99DM2tGyM","authorship_tag":"ABX9TyP8bgztCrxOlZvpuVo488Jv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## RAG Using Llama2 With Hugging Face"],"metadata":{"id":"BxlYlnBR9IS8"}},{"cell_type":"code","source":["# Install the PyPDF library, which provides tools for working with PDF documents\n","!pip install pypdf"],"metadata":{"id":"9c5RfWsn9H3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"o2fKJQGq8uje","executionInfo":{"status":"ok","timestamp":1722173753912,"user_tz":-60,"elapsed":9628,"user":{"displayName":"Nikhil Joshi","userId":"16466695062166081727"}}},"outputs":[],"source":["# Install the necessary libraries for the project:\n","# - transformers: for using pre-trained models from the Hugging Face library\n","# - einops: for flexible tensor operations\n","# - accelerate: for optimizing model training and inference\n","# - langchain: for building language models and chains\n","# - bitsandbytes: for efficient model training on GPUs\n","!pip install -q transformers einops accelerate langchain bitsandbytes"]},{"cell_type":"code","source":["# Install the llama-index library, which is used for indexing and querying large datasets\n","!pip install llama-index"],"metadata":{"id":"OEVOyuZRbQVk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install the following libraries:\n","# - llama-index-llms-llama-cpp: for integrating LLaMA models with llama-index using the llama-cpp backend\n","# - llama-index-embeddings-huggingface: for utilizing Hugging Face embeddings within llama-index\n","# - llama-index-llms-huggingface: for integrating Hugging Face language models with llama-index\n","!pip install llama-index-llms-llama-cpp llama-index-embeddings-huggingface llama-index-llms-huggingface"],"metadata":{"id":"5zDakodnbox_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Upgrade to the latest version of the langchain-community library.\n","# This library includes community-contributed tools and extensions for enhancing LangChain functionalities.\n","!pip install -U langchain-community"],"metadata":{"id":"qQJNTukHduPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Log in to the Hugging Face CLI (Command Line Interface).\n","# This command will prompt you to enter your Hugging Face credentials,\n","# allowing you to access and interact with Hugging Face's models and datasets.\n","!huggingface-cli login"],"metadata":{"id":"YvomHe63Dya3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display detailed information about the installed llama-index package,\n","# including its version, dependencies, and other metadata.\n","!pip show llama-index"],"metadata":{"id":"nG8CtTXnNL4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display detailed information about the installed langchain package,\n","# including its version, dependencies, and other metadata.\n","!pip show langchain"],"metadata":{"id":"ja1xwbyvT5We"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install the Streamlit library, which is used for building interactive web applications\n","# for data science and machine learning projects.\n","!pip install streamlit"],"metadata":{"id":"tdXH3iyg2lDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from llama_index.llms.huggingface import HuggingFaceLLM\n","from llama_index.core.prompts.prompts import SimpleInputPrompt\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from llama_index.core import ServiceContext, set_global_service_context, VectorStoreIndex, SimpleDirectoryReader\n","from llama_index.legacy.embeddings.langchain import LangchainEmbedding\n","import os\n","\n","# Set the Hugging Face API token as an environment variable.\n","# This token is used to authenticate and access Hugging Face models and datasets.\n","os.environ[\"HF_TOKEN\"] = ''\n","\n","\n","# Define the system prompt for the Q&A assistant\n","system_prompt=\"\"\"\n","You are a Q&A assistant. Your goal is to answer questions as\n","accurately as possible based on the instructions and context provided.\n","\"\"\"\n","\n","# Define the query wrapper prompt in the default format supportable by LLama2\n","query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\")\n","\n","# Initialize the Hugging Face LLaMA model with specific parameters\n","llm = HuggingFaceLLM(\n","    context_window=4096,\n","    max_new_tokens=256,\n","    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n","    system_prompt=system_prompt,\n","    query_wrapper_prompt=query_wrapper_prompt,\n","    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n","    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n","    device_map=\"cpu\",\n","    # Uncomment this if using CUDA to reduce memory usage\n","    # model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_4bit\":True}\n",")\n","\n","# Initialize the embedding model using Hugging Face embeddings\n","embed_model = LangchainEmbedding(\n","    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",")\n","\n","# Create a service context with default settings, including chunk size, LLM, and embedding model\n","service_context = ServiceContext.from_defaults(\n","    chunk_size=1024,\n","    llm=llm,\n","    embed_model=embed_model\n",")\n","\n","# Load documents from a specified directory using SimpleDirectoryReader\n","documents = SimpleDirectoryReader(\"/content/drive/MyDrive/Colab Notebooks/Langchain_RAG/data\").load_data()\n","\n","# Create a vector store index from the loaded documents using the service context\n","index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n","\n","# Create a query engine from the vector store index\n","query_engine = index.as_query_engine()\n","\n","# Perform a query to find the most ordered item\n","response = query_engine.query(\"which is the most ordered item?\")\n","print(response)"],"metadata":{"id":"3z-vio4UIo8E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Streamlit UI"],"metadata":{"id":"66E-1uyD3R4D"}},{"cell_type":"code","source":["import streamlit as st\n","import torch\n","from llama_index.llms.huggingface import HuggingFaceLLM\n","from llama_index.core.prompts.prompts import SimpleInputPrompt\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from llama_index.core import ServiceContext, set_global_service_context, VectorStoreIndex, SimpleDirectoryReader\n","from llama_index.legacy.embeddings.langchain import LangchainEmbedding\n","import os\n","from getpass import getpass\n","\n","# Set the Hugging Face API token as an environment variable\n","os.environ[\"HF_TOKEN\"] = getpass()\n","\n","# Define the system prompt for the Q&A assistant\n","system_prompt = \"\"\"\n","You are a Q&A assistant. Your goal is to answer questions as\n","accurately as possible based on the instructions and context provided.\n","\"\"\"\n","\n","# Define the query wrapper prompt in the default format supportable by LLama2\n","query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\")\n","\n","# Initialize the Hugging Face LLaMA model with specific parameters\n","llm = HuggingFaceLLM(\n","    context_window=4096,\n","    max_new_tokens=256,\n","    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n","    system_prompt=system_prompt,\n","    query_wrapper_prompt=query_wrapper_prompt,\n","    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n","    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n","    device_map=\"cpu\"\n",")\n","\n","# Initialize the embedding model using Hugging Face embeddings\n","embed_model = LangchainEmbedding(\n","    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",")\n","\n","# Create a service context with default settings, including chunk size, LLM, and embedding model\n","service_context = ServiceContext.from_defaults(\n","    chunk_size=1024,\n","    llm=llm,\n","    embed_model=embed_model\n",")\n","\n","# Streamlit UI setup\n","st.title(\"Document Q&A with LLama-2\")\n","\n","# File upload interface\n","uploaded_files = st.file_uploader(\"Upload your documents\", accept_multiple_files=True, type=[\"txt\", \"pdf\", \"docx\"])\n","\n","if uploaded_files:\n","    # Create a temporary directory for saving uploaded files\n","    temp_dir = \"temp_uploads\"\n","    os.makedirs(temp_dir, exist_ok=True)\n","\n","    # Save uploaded files to the temporary directory\n","    for uploaded_file in uploaded_files:\n","        with open(os.path.join(temp_dir, uploaded_file.name), \"wb\") as f:\n","            f.write(uploaded_file.getbuffer())\n","\n","    # Load documents from the temporary directory\n","    documents = SimpleDirectoryReader(temp_dir).load_data()\n","\n","    # Create a vector store index from the loaded documents using the service context\n","    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n","\n","    # Create a query engine from the vector store index\n","    query_engine = index.as_query_engine()\n","\n","    # Input field for the user to ask a question\n","    question = st.text_input(\"Ask a question about your documents\")\n","\n","    if question:\n","        # Get the response from the query engine and display it\n","        response = query_engine.query(question)\n","        st.write(\"Response:\", response)\n","\n","    # Clean up temporary files after processing\n","    for uploaded_file in uploaded_files:\n","        os.remove(os.path.join(temp_dir, uploaded_file.name))\n","else:\n","    st.write(\"Please upload documents to proceed.\")\n","\n","# Run the Streamlit app\n","!streamlit run app.py"],"metadata":{"id":"YI3UbfBbXPQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ekZXYhnmoAUm"},"execution_count":null,"outputs":[]}]}